{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9d0afd",
   "metadata": {},
   "source": [
    "# Formulaic Sentence Detection in Tang Dynasty Edicts\n",
    "\n",
    "This notebook identifies **formulaic sentences** (standardized phrases that appear across multiple edicts) versus **unique sentences** (edict-specific content) in Tang Dynasty imperial edicts using SIKU-BERT embeddings and cosine similarity.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Imperial edicts often follow conventional templates with formulaic language interspersed with context-specific content. This analysis helps distinguish between:\n",
    "\n",
    "- **Formulaic sentences**: Phrases that appear repeatedly across different edicts with high semantic similarity (≥ threshold)\n",
    "- **Unique sentences**: Edict-specific content with no close counterparts in other documents\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Segmentation**: Split each edict into sentences based on Chinese punctuation marks (。；！？)\n",
    "2. **Embedding**: Generate SIKU-BERT embeddings for all sentences\n",
    "3. **Similarity Analysis**: Compute cosine similarity between sentences across different edicts\n",
    "4. **Classification**: Mark sentences as formulaic if similarity ≥ threshold (default 0.85)\n",
    "5. **Visualization**: Export formatted texts with bold highlighting for formulaic sentences\n",
    "6. **Interactive Exploration**: Compare edicts dynamically with selective highlighting\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Key parameters you can adjust:\n",
    "- `EDICT_TYPE`: The document type to analyze (e.g., '册文', '即位赦', '改元赦')\n",
    "- `SIMILARITY_THRESHOLD`: Minimum cosine similarity to consider sentences formulaic (0.0-1.0)\n",
    "- `MIN_SENTENCE_LENGTH`: Minimum characters for a valid sentence\n",
    "\n",
    "## Outputs\n",
    "\n",
    "The notebook generates:\n",
    "1. **Markdown file**: Formatted texts with formulaic sentences in **bold**\n",
    "2. **CSV file**: Detailed sentence-level data with similarity scores\n",
    "3. **Interactive widget**: Dynamic edict comparison with highlighting\n",
    "\n",
    "Let's begin the analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4aa4f",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install all required Python packages for the analysis. This includes:\n",
    "- **pandas**: Data manipulation and CSV handling\n",
    "- **numpy**: Numerical computations\n",
    "- **torch**: PyTorch for running SIKU-BERT model\n",
    "- **transformers**: HuggingFace library for BERT models\n",
    "- **scikit-learn**: Cosine similarity computation\n",
    "- **tqdm**: Progress bars for long operations\n",
    "- **ipywidgets**: Interactive widgets for edict exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d6969de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (2.3.5)\n",
      "Requirement already satisfied: torch in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: scikit-learn in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: tqdm in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: ipywidgets in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (8.1.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipywidgets) (9.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yegor/anaconda3/envs/tang_edicts/lib/python3.13/site-packages (from requests->transformers) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy torch transformers scikit-learn tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc46d8",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe81b7",
   "metadata": {},
   "source": [
    "Import all necessary libraries and configure display settings. We suppress warnings to keep output clean and set pandas to display full column information for better visibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bc9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e683c",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196e15d",
   "metadata": {},
   "source": [
    "Set analysis parameters. **EDICT_TYPE** determines which document type to analyze. **SIMILARITY_THRESHOLD** controls how similar sentences must be to count as formulaic (0.85 = 85% similarity). Lower thresholds will identify more sentences as formulaic; higher thresholds will be more conservative. The notebook will automatically detect GPU availability for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e5889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Edict type: 册文\n",
      "  Similarity threshold: 0.85\n",
      "  Min sentence length: 5\n",
      "  Model path: ./sikubert\n",
      "  Output file: formulaic_analysis_册文.md\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EDICT_TYPE = '册文'  # Change to analyze different edict types\n",
    "SIMILARITY_THRESHOLD = 0.85  # Sentences with similarity ≥ this are considered formulaic\n",
    "MIN_SENTENCE_LENGTH = 5  # Minimum characters for a valid sentence\n",
    "MODEL_PATH = './sikubert'  # Path to local SIKU-BERT model\n",
    "OUTPUT_FILE = f'formulaic_analysis_{EDICT_TYPE}.md'  # Output Markdown file\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Edict type: {EDICT_TYPE}\")\n",
    "print(f\"  Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  Min sentence length: {MIN_SENTENCE_LENGTH}\")\n",
    "print(f\"  Model path: {MODEL_PATH}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617303ba",
   "metadata": {},
   "source": [
    "## 3. Load SIKU-BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f71c44",
   "metadata": {},
   "source": [
    "Load the SIKU-BERT model from the local path. SIKU-BERT is a BERT model specifically trained on classical Chinese texts from the Siku Quanshu (四庫全書), making it ideal for analyzing Tang Dynasty documents. The model is set to evaluation mode (no training) and moved to GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13543919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SIKU-BERT model from ./sikubert...\n",
      "✅ Model loaded successfully!\n",
      "   Model parameters: 108,920,832\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading SIKU-BERT model from {MODEL_PATH}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce2487",
   "metadata": {},
   "source": [
    "## 4. Define Embedding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf17ec2",
   "metadata": {},
   "source": [
    "Define the embedding generation function. This function converts text strings into dense vector representations (embeddings) using SIKU-BERT. We use the [CLS] token embedding as the sentence representation, which captures the overall semantic meaning. Batch processing improves efficiency when encoding many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485a7cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding function defined\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Generate SIKU-BERT embeddings for a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings (num_texts, embedding_dim)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Use [CLS] token embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "print(\"✅ Embedding function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c8cac",
   "metadata": {},
   "source": [
    "## 5. Load and Segment Edicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ef2d8",
   "metadata": {},
   "source": [
    "Load the edict dataset from CSV and filter by document type. The CSV file contains extracted Tang Dynasty edicts with punctuated text. We select only edicts matching the specified type (e.g., '册文') and ensure they have valid text content. The notebook displays all available edicts for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5d3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edicts from extracted_edicts_punc.csv...\n",
      "\n",
      "Found 56 edicts of type '册文'\n",
      "\n",
      "Edicts:\n",
      "  1. 册晉王為皇太子文\n",
      "  2. 册代王為皇太子文\n",
      "  3. 册平王為皇太子文\n",
      "  4. 册郢王為皇太子文\n",
      "  5. 册忠王為皇太子文\n",
      "  6. 册成王為皇太子文\n",
      "  7. 册雍王為皇太子文\n",
      "  8. 册廣陵郡王為皇太子文\n",
      "  9. 册鄧王為皇太子文\n",
      "  10. 册遂王為皇太子文\n",
      "  11. 册景王為皇太子文\n",
      "  12. 册魯王為皇太子文\n",
      "  13. 册德王為皇太子文\n",
      "  14. 册梁州都督漢王元昌文\n",
      "  15. 册徐州都督徐王元禮文\n",
      "  16. 册荆州都督荆王元景文\n",
      "  17. 册潞州都督韓王元嘉文\n",
      "  18. 册遂州都督彭王元則文\n",
      "  19. 册雍州牧左武侯大將軍越王泰改封魏王文\n",
      "  20. 册洺州刺史郯王惲改封蔣王文\n",
      "  21. 册岐州刺史許王元祥改封江王文\n",
      "  22. 册揚州都督沛王文\n",
      "  23. 册殷王旭輪文\n",
      "  24. 册冀王輪文\n",
      "  25. 册汴王邕文\n",
      "  26. 册魯王永文\n",
      "  27. 册魏王佾文\n",
      "  28. 册凉王侹文\n",
      "  29. 册蜀王佶文\n",
      "  30. 册景王祕文\n",
      "  31. 册輝王祚文\n",
      "  32. 册祁王祺文\n",
      "  33. 册雅王禎文\n",
      "  34. 册瓊王祥文\n",
      "  35. 册信成公主文\n",
      "  36. 册昌樂公主文\n",
      "  37. 册髙都公主文\n",
      "  38. 册建平公主文\n",
      "  39. 册永寧公主文\n",
      "  40. 册臨晉公主文\n",
      "  41. 册真陽公主文\n",
      "  42. 册華陽公主文\n",
      "  43. 册益昌公主文\n",
      "  44. 册突厥李思摩為可汗文\n",
      "  45. 册踈勒國王文\n",
      "  46. 册莫賀咄屯為順義王文\n",
      "  47. 册突厥苾伽骨吐禄為可汗文\n",
      "  48. 册骨吐祿三姓毗方伽頡利發文\n",
      "  49. 册突騎施黒姓可汗文\n",
      "  50. 册囬紇為英武威逺可汗文\n",
      "  51. 册新羅王金乾運文\n",
      "  52. 册新羅王太妃文\n",
      "  53. 册新囬鶻可汗文\n",
      "  54. 册囬鶻可汗加號文\n",
      "  55. 册囬鶻彰信可汗文\n",
      "  56. 大中十一年册囬鶻可汗文\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading edicts from extracted_edicts_punc.csv...\")\n",
    "\n",
    "df_all = pd.read_csv('extracted_edicts_punc.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Filter by edict type\n",
    "df_edicts = df_all[\n",
    "    (df_all['document_type'] == EDICT_TYPE) & \n",
    "    (df_all['text_contents_punctuated'].notna())\n",
    "].copy()\n",
    "\n",
    "df_edicts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"\\nFound {len(df_edicts)} edicts of type '{EDICT_TYPE}'\")\n",
    "\n",
    "if len(df_edicts) < 2:\n",
    "    print(\"\\n⚠️  WARNING: Need at least 2 edicts for comparison!\")\n",
    "    print(\"   Please choose a different EDICT_TYPE with more examples.\")\n",
    "else:\n",
    "    print(\"\\nEdicts:\")\n",
    "    for idx, row in df_edicts.iterrows():\n",
    "        print(f\"  {idx+1}. {row['text_title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73696b",
   "metadata": {},
   "source": [
    "Segment each edict into individual sentences. Chinese sentences are identified by major punctuation marks: 。(period), ；(semicolon), ！(exclamation), and ？(question mark). We also track the position of each sentence within the original text for later reconstruction with formatting. Short fragments below MIN_SENTENCE_LENGTH are filtered out to avoid noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22bb1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmenting edicts into sentences...\n",
      "✅ Segmentation complete!\n",
      "   Total sentences: 571\n",
      "   Sentences per edict:\n",
      "     册晉王為皇太子文: 7 sentences\n",
      "     册代王為皇太子文: 11 sentences\n",
      "     册平王為皇太子文: 13 sentences\n",
      "     册郢王為皇太子文: 6 sentences\n",
      "     册忠王為皇太子文: 15 sentences\n",
      "     册成王為皇太子文: 13 sentences\n",
      "     册雍王為皇太子文: 11 sentences\n",
      "     册廣陵郡王為皇太子文: 7 sentences\n",
      "     册鄧王為皇太子文: 10 sentences\n",
      "     册遂王為皇太子文: 7 sentences\n",
      "     册景王為皇太子文: 6 sentences\n",
      "     册魯王為皇太子文: 7 sentences\n",
      "     册德王為皇太子文: 10 sentences\n",
      "     册梁州都督漢王元昌文: 11 sentences\n",
      "     册徐州都督徐王元禮文: 8 sentences\n",
      "     册荆州都督荆王元景文: 8 sentences\n",
      "     册潞州都督韓王元嘉文: 15 sentences\n",
      "     册遂州都督彭王元則文: 12 sentences\n",
      "     册雍州牧左武侯大將軍越王泰改封魏王文: 10 sentences\n",
      "     册洺州刺史郯王惲改封蔣王文: 8 sentences\n",
      "     册岐州刺史許王元祥改封江王文: 11 sentences\n",
      "     册揚州都督沛王文: 12 sentences\n",
      "     册殷王旭輪文: 5 sentences\n",
      "     册冀王輪文: 18 sentences\n",
      "     册汴王邕文: 13 sentences\n",
      "     册魯王永文: 10 sentences\n",
      "     册魏王佾文: 11 sentences\n",
      "     册凉王侹文: 9 sentences\n",
      "     册蜀王佶文: 6 sentences\n",
      "     册景王祕文: 7 sentences\n",
      "     册輝王祚文: 8 sentences\n",
      "     册祁王祺文: 7 sentences\n",
      "     册雅王禎文: 7 sentences\n",
      "     册瓊王祥文: 12 sentences\n",
      "     册信成公主文: 11 sentences\n",
      "     册昌樂公主文: 8 sentences\n",
      "     册髙都公主文: 6 sentences\n",
      "     册建平公主文: 6 sentences\n",
      "     册永寧公主文: 13 sentences\n",
      "     册臨晉公主文: 7 sentences\n",
      "     册真陽公主文: 7 sentences\n",
      "     册華陽公主文: 7 sentences\n",
      "     册益昌公主文: 11 sentences\n",
      "     册突厥李思摩為可汗文: 10 sentences\n",
      "     册踈勒國王文: 6 sentences\n",
      "     册莫賀咄屯為順義王文: 3 sentences\n",
      "     册突厥苾伽骨吐禄為可汗文: 7 sentences\n",
      "     册骨吐祿三姓毗方伽頡利發文: 10 sentences\n",
      "     册突騎施黒姓可汗文: 11 sentences\n",
      "     册囬紇為英武威逺可汗文: 12 sentences\n",
      "     册新羅王金乾運文: 4 sentences\n",
      "     册新羅王太妃文: 5 sentences\n",
      "     册新囬鶻可汗文: 23 sentences\n",
      "     册囬鶻可汗加號文: 15 sentences\n",
      "     册囬鶻彰信可汗文: 36 sentences\n",
      "     大中十一年册囬鶻可汗文: 22 sentences\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSegmenting edicts into sentences...\")\n",
    "\n",
    "def segment_sentences(text):\n",
    "    \"\"\"\n",
    "    Segment text into sentences based on Chinese punctuation.\n",
    "    \n",
    "    Returns:\n",
    "        List of (sentence_text, start_pos, end_pos) tuples\n",
    "    \"\"\"\n",
    "    # Split by major delimiters\n",
    "    parts = re.split(r'([。；！？])', text)\n",
    "    \n",
    "    # Reconstruct sentences with delimiters\n",
    "    sentences = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for i in range(0, len(parts)-1, 2):\n",
    "        if i+1 < len(parts):\n",
    "            sent = parts[i] + parts[i+1]\n",
    "            sent = sent.strip()\n",
    "            \n",
    "            if len(sent) >= MIN_SENTENCE_LENGTH:\n",
    "                # Find position in original text\n",
    "                start_pos = text.find(sent, current_pos)\n",
    "                if start_pos == -1:\n",
    "                    start_pos = current_pos\n",
    "                end_pos = start_pos + len(sent)\n",
    "                \n",
    "                sentences.append((sent, start_pos, end_pos))\n",
    "                current_pos = end_pos\n",
    "    \n",
    "    # Handle last sentence without delimiter\n",
    "    if len(parts) % 2 == 1:\n",
    "        last_sent = parts[-1].strip()\n",
    "        if len(last_sent) >= MIN_SENTENCE_LENGTH:\n",
    "            start_pos = text.find(last_sent, current_pos)\n",
    "            if start_pos == -1:\n",
    "                start_pos = current_pos\n",
    "            end_pos = start_pos + len(last_sent)\n",
    "            sentences.append((last_sent, start_pos, end_pos))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Segment all edicts\n",
    "edict_sentences = []  # List of dicts with metadata\n",
    "\n",
    "for idx, row in df_edicts.iterrows():\n",
    "    edict_title = row['text_title']\n",
    "    full_text = row['text_contents_punctuated']\n",
    "    \n",
    "    sentences = segment_sentences(full_text)\n",
    "    \n",
    "    for sent_idx, (sent_text, start_pos, end_pos) in enumerate(sentences):\n",
    "        edict_sentences.append({\n",
    "            'edict_idx': idx,\n",
    "            'edict_title': edict_title,\n",
    "            'sentence_idx': sent_idx,\n",
    "            'sentence_text': sent_text,\n",
    "            'start_pos': start_pos,\n",
    "            'end_pos': end_pos,\n",
    "            'full_text': full_text\n",
    "        })\n",
    "\n",
    "df_sentences = pd.DataFrame(edict_sentences)\n",
    "\n",
    "print(f\"✅ Segmentation complete!\")\n",
    "print(f\"   Total sentences: {len(df_sentences)}\")\n",
    "print(f\"   Sentences per edict:\")\n",
    "for idx, row in df_edicts.iterrows():\n",
    "    count = len(df_sentences[df_sentences['edict_idx'] == idx])\n",
    "    print(f\"     {row['text_title']}: {count} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d8065",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf3900",
   "metadata": {},
   "source": [
    "Generate SIKU-BERT embeddings for all sentences in the dataset. This converts each sentence from text into a numerical vector that captures its semantic meaning. The embedding process may take several minutes depending on the number of sentences and whether GPU acceleration is available. Progress is shown with a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6253da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SIKU-BERT embeddings for 571 sentences...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  78%|███████▊  | 28/36 [01:58<00:33,  4.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m sentence_texts = df_sentences[\u001b[33m'\u001b[39m\u001b[33msentence_text\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m embeddings = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Embeddings generated!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m     26\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Get embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Use [CLS] token embedding\u001b[39;00m\n\u001b[32m     32\u001b[39m embeddings = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :].cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1000\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    998\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1014\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:650\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    646\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:558\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    556\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    557\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    567\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:488\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    486\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    487\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    498\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:363\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().forward(\n\u001b[32m    351\u001b[39m         hidden_states,\n\u001b[32m    352\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m         cache_position,\n\u001b[32m    358\u001b[39m     )\n\u001b[32m    360\u001b[39m bsz, tgt_len, _ = hidden_states.size()\n\u001b[32m    362\u001b[39m query_layer = (\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    364\u001b[39m )\n\u001b[32m    366\u001b[39m is_updated = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    367\u001b[39m is_cross_attention = encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tang_edicts/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"Generating SIKU-BERT embeddings for {len(df_sentences)} sentences...\\n\")\n",
    "\n",
    "# Get all sentence texts\n",
    "sentence_texts = df_sentences['sentence_text'].tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = get_embeddings(sentence_texts, batch_size=16)\n",
    "\n",
    "print(f\"\\n✅ Embeddings generated!\")\n",
    "print(f\"   Shape: {embeddings.shape}\")\n",
    "print(f\"   Memory: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca95ea9",
   "metadata": {},
   "source": [
    "## 7. Compute Similarity and Identify Formulaic Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b2a1e",
   "metadata": {},
   "source": [
    "Compute pairwise cosine similarity between all sentences and identify formulaic patterns. For each sentence, we find its most similar counterpart from **other edicts** (not from the same edict). If the maximum similarity ≥ threshold, the sentence is classified as formulaic. This approach ensures we're detecting cross-document patterns rather than internal repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb94e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computing sentence similarities...\\n\")\n",
    "\n",
    "# Compute full similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(f\"✅ Similarity matrix computed: {similarity_matrix.shape}\\n\")\n",
    "\n",
    "# For each sentence, find max similarity with sentences from OTHER edicts\n",
    "formulaic_flags = []\n",
    "max_similarities = []\n",
    "best_matches = []\n",
    "\n",
    "for i in tqdm(range(len(df_sentences)), desc=\"Identifying formulaic sentences\"):\n",
    "    current_edict_idx = df_sentences.iloc[i]['edict_idx']\n",
    "    \n",
    "    # Find indices of sentences from other edicts\n",
    "    other_edict_mask = df_sentences['edict_idx'] != current_edict_idx\n",
    "    other_edict_indices = df_sentences[other_edict_mask].index.tolist()\n",
    "    \n",
    "    if len(other_edict_indices) == 0:\n",
    "        # Only one edict - cannot compare\n",
    "        formulaic_flags.append(False)\n",
    "        max_similarities.append(0.0)\n",
    "        best_matches.append(None)\n",
    "        continue\n",
    "    \n",
    "    # Get similarities to other edicts\n",
    "    similarities_to_others = similarity_matrix[i, other_edict_indices]\n",
    "    \n",
    "    # Find maximum similarity\n",
    "    max_sim = similarities_to_others.max()\n",
    "    max_sim_idx_in_others = similarities_to_others.argmax()\n",
    "    best_match_idx = other_edict_indices[max_sim_idx_in_others]\n",
    "    \n",
    "    # Determine if formulaic\n",
    "    is_formulaic = max_sim >= SIMILARITY_THRESHOLD\n",
    "    \n",
    "    formulaic_flags.append(is_formulaic)\n",
    "    max_similarities.append(max_sim)\n",
    "    best_matches.append(best_match_idx)\n",
    "\n",
    "# Add to dataframe\n",
    "df_sentences['is_formulaic'] = formulaic_flags\n",
    "df_sentences['max_similarity'] = max_similarities\n",
    "df_sentences['best_match_idx'] = best_matches\n",
    "\n",
    "# Statistics\n",
    "num_formulaic = df_sentences['is_formulaic'].sum()\n",
    "num_unique = len(df_sentences) - num_formulaic\n",
    "\n",
    "print(f\"\\n✅ Formulaic identification complete!\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total sentences: {len(df_sentences)}\")\n",
    "print(f\"  Formulaic sentences (≥{SIMILARITY_THRESHOLD} similarity): {num_formulaic} ({num_formulaic/len(df_sentences)*100:.1f}%)\")\n",
    "print(f\"  Unique sentences: {num_unique} ({num_unique/len(df_sentences)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFormulaic sentences by edict:\")\n",
    "for idx, row in df_edicts.iterrows():\n",
    "    edict_sents = df_sentences[df_sentences['edict_idx'] == idx]\n",
    "    num_form = edict_sents['is_formulaic'].sum()\n",
    "    total = len(edict_sents)\n",
    "    print(f\"  {row['text_title']}: {num_form}/{total} ({num_form/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eae439",
   "metadata": {},
   "source": [
    "## 8. Display Sample Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977467dd",
   "metadata": {},
   "source": [
    "Display representative examples of formulaic and unique sentences. Formulaic examples are sorted by similarity score to show the strongest patterns. For each formulaic sentence, we show both the original sentence and its best match from another edict. Unique examples demonstrate sentences with no close counterparts across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9edb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"SAMPLE FORMULAIC SENTENCES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if num_formulaic > 0:\n",
    "    formulaic_sents = df_sentences[df_sentences['is_formulaic']].nlargest(5, 'max_similarity')\n",
    "    \n",
    "    for i, (idx, row) in enumerate(formulaic_sents.iterrows(), 1):\n",
    "        print(f\"\\n{'-' * 100}\")\n",
    "        print(f\"Example #{i}\")\n",
    "        print(f\"{'-' * 100}\")\n",
    "        print(f\"Edict: {row['edict_title']}\")\n",
    "        print(f\"Similarity: {row['max_similarity']:.3f}\")\n",
    "        print(f\"\\nSentence:\")\n",
    "        print(f\"  {row['sentence_text']}\")\n",
    "        \n",
    "        if row['best_match_idx'] is not None:\n",
    "            match_row = df_sentences.iloc[row['best_match_idx']]\n",
    "            print(f\"\\nBest match (from {match_row['edict_title']}):\")\n",
    "            print(f\"  {match_row['sentence_text']}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No formulaic sentences found with current threshold.\")\n",
    "    print(f\"   Consider lowering SIMILARITY_THRESHOLD (current: {SIMILARITY_THRESHOLD})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SAMPLE UNIQUE SENTENCES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if num_unique > 0:\n",
    "    unique_sents = df_sentences[~df_sentences['is_formulaic']].nsmallest(5, 'max_similarity')\n",
    "    \n",
    "    for i, (idx, row) in enumerate(unique_sents.iterrows(), 1):\n",
    "        print(f\"\\n{'-' * 100}\")\n",
    "        print(f\"Example #{i}\")\n",
    "        print(f\"{'-' * 100}\")\n",
    "        print(f\"Edict: {row['edict_title']}\")\n",
    "        print(f\"Max similarity: {row['max_similarity']:.3f}\")\n",
    "        print(f\"\\nSentence:\")\n",
    "        print(f\"  {row['sentence_text']}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No unique sentences found - all sentences are formulaic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de30b3",
   "metadata": {},
   "source": [
    "## 9. Export to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79e273",
   "metadata": {},
   "source": [
    "Export results to a formatted Markdown file for human reading. Each edict is reconstructed with **formulaic sentences in bold** and unique sentences in regular text. The file includes statistics for each edict and a summary at the top. This provides an easy way to read and analyze the complete texts with visual distinction between formulaic and unique content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Exporting results to {OUTPUT_FILE}...\\n\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    # Write header\n",
    "    f.write(f\"# Formulaic Sentence Analysis: {EDICT_TYPE}\\n\\n\")\n",
    "    f.write(f\"**Analysis Parameters:**\\n\")\n",
    "    f.write(f\"- Edict Type: {EDICT_TYPE}\\n\")\n",
    "    f.write(f\"- Number of Edicts: {len(df_edicts)}\\n\")\n",
    "    f.write(f\"- Similarity Threshold: {SIMILARITY_THRESHOLD}\\n\")\n",
    "    f.write(f\"- Total Sentences: {len(df_sentences)}\\n\")\n",
    "    f.write(f\"- Formulaic Sentences: {num_formulaic} ({num_formulaic/len(df_sentences)*100:.1f}%)\\n\")\n",
    "    f.write(f\"- Unique Sentences: {num_unique} ({num_unique/len(df_sentences)*100:.1f}%)\\n\")\n",
    "    f.write(f\"\\n**Legend:**\\n\")\n",
    "    f.write(f\"- **Bold text** = Formulaic sentence (similar to sentences in other edicts)\\n\")\n",
    "    f.write(f\"- Regular text = Unique sentence (no close counterparts in other edicts)\\n\")\n",
    "    f.write(f\"\\n---\\n\\n\")\n",
    "    \n",
    "    # Process each edict\n",
    "    for edict_idx, edict_row in df_edicts.iterrows():\n",
    "        edict_title = edict_row['text_title']\n",
    "        full_text = edict_row['text_contents_punctuated']\n",
    "        \n",
    "        # Get sentences for this edict\n",
    "        edict_sents = df_sentences[df_sentences['edict_idx'] == edict_idx].sort_values('sentence_idx')\n",
    "        \n",
    "        # Write edict header\n",
    "        f.write(f\"## {edict_title}\\n\\n\")\n",
    "        \n",
    "        # Statistics for this edict\n",
    "        num_form_edict = edict_sents['is_formulaic'].sum()\n",
    "        total_sents_edict = len(edict_sents)\n",
    "        f.write(f\"*Formulaic: {num_form_edict}/{total_sents_edict} sentences ({num_form_edict/total_sents_edict*100:.1f}%)*\\n\\n\")\n",
    "        \n",
    "        # Reconstruct text with formatting\n",
    "        # We'll process the full text and apply bold formatting\n",
    "        formatted_text = full_text\n",
    "        \n",
    "        # Sort sentences by position (reverse order for string replacement)\n",
    "        sorted_sents = edict_sents.sort_values('start_pos', ascending=False)\n",
    "        \n",
    "        for _, sent_row in sorted_sents.iterrows():\n",
    "            sent_text = sent_row['sentence_text']\n",
    "            start_pos = sent_row['start_pos']\n",
    "            end_pos = sent_row['end_pos']\n",
    "            is_formulaic = sent_row['is_formulaic']\n",
    "            \n",
    "            # Extract original sentence from full text\n",
    "            original_sent = full_text[start_pos:end_pos]\n",
    "            \n",
    "            # Apply formatting\n",
    "            if is_formulaic:\n",
    "                # Bold for formulaic\n",
    "                formatted_sent = f\"**{original_sent}**\"\n",
    "            else:\n",
    "                # Keep as-is for unique\n",
    "                formatted_sent = original_sent\n",
    "            \n",
    "            # Replace in formatted text\n",
    "            formatted_text = formatted_text[:start_pos] + formatted_sent + formatted_text[end_pos:]\n",
    "        \n",
    "        # Write formatted text\n",
    "        f.write(formatted_text)\n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "\n",
    "print(f\"✅ Export complete!\")\n",
    "print(f\"\\nOutput file: {OUTPUT_FILE}\")\n",
    "print(f\"\\nThe file contains:\")\n",
    "print(f\"  - Complete text of all {len(df_edicts)} edicts\")\n",
    "print(f\"  - Formulaic sentences in **bold**\")\n",
    "print(f\"  - Unique sentences in regular text\")\n",
    "print(f\"  - Statistics for each edict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd2474",
   "metadata": {},
   "source": [
    "## 10. Create Detailed CSV Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409f654",
   "metadata": {},
   "source": [
    "Create a detailed CSV report with sentence-level analysis data. This machine-readable file contains every sentence with its classification (formulaic/unique), similarity score, and best matching sentence from other edicts. Useful for further statistical analysis, data processing, or integration with other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed sentence-level data\n",
    "csv_file = f'formulaic_sentences_{EDICT_TYPE}.csv'\n",
    "\n",
    "# Prepare export data\n",
    "export_df = df_sentences[[\n",
    "    'edict_title', 'sentence_idx', 'sentence_text', \n",
    "    'is_formulaic', 'max_similarity'\n",
    "]].copy()\n",
    "\n",
    "# Add best match information\n",
    "best_match_titles = []\n",
    "best_match_texts = []\n",
    "\n",
    "for idx, row in df_sentences.iterrows():\n",
    "    if row['best_match_idx'] is not None and pd.notna(row['best_match_idx']):\n",
    "        match_row = df_sentences.iloc[int(row['best_match_idx'])]\n",
    "        best_match_titles.append(match_row['edict_title'])\n",
    "        best_match_texts.append(match_row['sentence_text'])\n",
    "    else:\n",
    "        best_match_titles.append('')\n",
    "        best_match_texts.append('')\n",
    "\n",
    "export_df['best_match_edict'] = best_match_titles\n",
    "export_df['best_match_sentence'] = best_match_texts\n",
    "\n",
    "export_df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Detailed CSV report saved: {csv_file}\")\n",
    "print(f\"\\nCSV contains sentence-level data:\")\n",
    "print(f\"  - Edict title\")\n",
    "print(f\"  - Sentence index\")\n",
    "print(f\"  - Sentence text\")\n",
    "print(f\"  - Formulaic flag\")\n",
    "print(f\"  - Maximum similarity score\")\n",
    "print(f\"  - Best matching edict and sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb32e4",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238ef60",
   "metadata": {},
   "source": [
    "Display comprehensive summary statistics for the entire analysis. This includes overall formulaic/unique ratios, similarity score distributions, and per-edict breakdowns. The interpretation section helps contextualize the results by explaining what different levels of formulaic content might indicate about the document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb01a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FORMULAIC SENTENCE ANALYSIS - SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n📊 Dataset:\")\n",
    "print(f\"   Edict type: {EDICT_TYPE}\")\n",
    "print(f\"   Total edicts: {len(df_edicts)}\")\n",
    "print(f\"   Total sentences: {len(df_sentences)}\")\n",
    "\n",
    "print(f\"\\n🎯 Detection Results:\")\n",
    "print(f\"   Similarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"   Formulaic sentences: {num_formulaic} ({num_formulaic/len(df_sentences)*100:.1f}%)\")\n",
    "print(f\"   Unique sentences: {num_unique} ({num_unique/len(df_sentences)*100:.1f}%)\")\n",
    "\n",
    "if num_formulaic > 0:\n",
    "    formulaic_df = df_sentences[df_sentences['is_formulaic']]\n",
    "    print(f\"\\n📈 Similarity Metrics (Formulaic Sentences):\")\n",
    "    print(f\"   Mean similarity: {formulaic_df['max_similarity'].mean():.3f}\")\n",
    "    print(f\"   Median similarity: {formulaic_df['max_similarity'].median():.3f}\")\n",
    "    print(f\"   Min similarity: {formulaic_df['max_similarity'].min():.3f}\")\n",
    "    print(f\"   Max similarity: {formulaic_df['max_similarity'].max():.3f}\")\n",
    "\n",
    "print(f\"\\n📝 Per-Edict Breakdown:\")\n",
    "for idx, row in df_edicts.iterrows():\n",
    "    edict_sents = df_sentences[df_sentences['edict_idx'] == idx]\n",
    "    num_form = edict_sents['is_formulaic'].sum()\n",
    "    total = len(edict_sents)\n",
    "    avg_sim = edict_sents[edict_sents['is_formulaic']]['max_similarity'].mean()\n",
    "    \n",
    "    print(f\"\\n   {row['text_title']}:\")\n",
    "    print(f\"     Total sentences: {total}\")\n",
    "    print(f\"     Formulaic: {num_form} ({num_form/total*100:.1f}%)\")\n",
    "    if num_form > 0:\n",
    "        print(f\"     Avg similarity: {avg_sim:.3f}\")\n",
    "\n",
    "print(f\"\\n📁 Output Files:\")\n",
    "print(f\"   1. {OUTPUT_FILE} - Formatted Markdown with bold highlighting\")\n",
    "print(f\"   2. {csv_file} - Detailed sentence-level CSV data\")\n",
    "\n",
    "print(f\"\\n💡 Interpretation:\")\n",
    "if num_formulaic > len(df_sentences) * 0.5:\n",
    "    print(f\"   High formulaic content ({num_formulaic/len(df_sentences)*100:.0f}%) suggests strong\")\n",
    "    print(f\"   adherence to template conventions in {EDICT_TYPE} edicts.\")\n",
    "elif num_formulaic > len(df_sentences) * 0.3:\n",
    "    print(f\"   Moderate formulaic content ({num_formulaic/len(df_sentences)*100:.0f}%) indicates a mix\")\n",
    "    print(f\"   of standard phrasing and edict-specific content.\")\n",
    "else:\n",
    "    print(f\"   Low formulaic content ({num_formulaic/len(df_sentences)*100:.0f}%) suggests high\")\n",
    "    print(f\"   variability and context-specific language in these edicts.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✅ Analysis complete!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c854a",
   "metadata": {},
   "source": [
    "## 12. Visualize Edict Lengths and Formulaic Content\n",
    "\n",
    "Create a visual comparison of edict lengths showing the distribution of formulaic vs. unique content. Each edict is represented as a horizontal bar where colored segments indicate formulaic passages and white segments show unique content. This provides an at-a-glance view of how standardized language is distributed across different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ac89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Configure matplotlib to display Chinese characters properly\n",
    "# Try to find available Chinese fonts on the system\n",
    "def get_chinese_font():\n",
    "    \"\"\"Find an available Chinese font on the system.\"\"\"\n",
    "    chinese_fonts = [\n",
    "        'WenQuanYi Micro Hei',  # Common on Linux\n",
    "        'WenQuanYi Zen Hei',\n",
    "        'Noto Sans CJK SC',\n",
    "        'Noto Sans CJK TC',\n",
    "        'Droid Sans Fallback',\n",
    "        'SimHei',  # Windows\n",
    "        'Arial Unicode MS',  # macOS\n",
    "        'Microsoft YaHei',\n",
    "        'STHeiti',\n",
    "    ]\n",
    "    \n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    for font in chinese_fonts:\n",
    "        if font in available_fonts:\n",
    "            print(f\"Using Chinese font: {font}\")\n",
    "            return font\n",
    "    \n",
    "    # If no Chinese font found, try the first CJK font available\n",
    "    for font_name in available_fonts:\n",
    "        if any(keyword in font_name.lower() for keyword in ['cjk', 'chinese', 'han', 'hei', 'song', 'kai']):\n",
    "            print(f\"Using Chinese font: {font_name}\")\n",
    "            return font_name\n",
    "    \n",
    "    print(\"⚠️  Warning: No Chinese font detected. Chinese characters may not display correctly.\")\n",
    "    print(\"   On Linux, install fonts with: sudo apt-get install fonts-wqy-microhei fonts-wqy-zenhei\")\n",
    "    return 'DejaVu Sans'\n",
    "\n",
    "chinese_font = get_chinese_font()\n",
    "plt.rcParams['font.sans-serif'] = [chinese_font, 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display\n",
    "\n",
    "print(\"Creating edict length visualization with formulaic content...\\n\")\n",
    "\n",
    "# Prepare data for visualization\n",
    "edict_viz_data = []\n",
    "\n",
    "for edict_idx, edict_row in df_edicts.iterrows():\n",
    "    # Get text without punctuation from text_contents column\n",
    "    text_no_punc = edict_row['text_contents'] if 'text_contents' in edict_row else edict_row['text_contents_punctuated']\n",
    "    # Remove common Chinese punctuation marks\n",
    "    for punct in ['。', '；', '！', '？', '，', '、', '：', '「', '」', '『', '』', '（', '）', '《', '》']:\n",
    "        text_no_punc = text_no_punc.replace(punct, '')\n",
    "    \n",
    "    total_length = len(text_no_punc)\n",
    "    \n",
    "    # Get sentences for this edict\n",
    "    edict_sents = df_sentences[df_sentences['edict_idx'] == edict_idx].sort_values('start_pos')\n",
    "    \n",
    "    # Build segments for visualization\n",
    "    # Each segment is (start_char, end_char, is_formulaic)\n",
    "    segments = []\n",
    "    \n",
    "    for _, sent_row in edict_sents.iterrows():\n",
    "        # Calculate positions without punctuation\n",
    "        full_text_with_punc = edict_row['text_contents_punctuated']\n",
    "        start_pos_punc = sent_row['start_pos']\n",
    "        end_pos_punc = sent_row['end_pos']\n",
    "        \n",
    "        # Count characters without punctuation up to this point\n",
    "        text_before = full_text_with_punc[:start_pos_punc]\n",
    "        start_pos_no_punc = len(text_before)\n",
    "        for punct in ['。', '；', '！', '？', '，', '、', '：', '「', '」', '『', '』', '（', '）', '《', '》']:\n",
    "            start_pos_no_punc -= text_before.count(punct)\n",
    "        \n",
    "        sent_text = full_text_with_punc[start_pos_punc:end_pos_punc]\n",
    "        sent_length_no_punc = len(sent_text)\n",
    "        for punct in ['。', '；', '！', '？', '，', '、', '：', '「', '」', '『', '』', '（', '）', '《', '》']:\n",
    "            sent_length_no_punc -= sent_text.count(punct)\n",
    "        \n",
    "        end_pos_no_punc = start_pos_no_punc + sent_length_no_punc\n",
    "        \n",
    "        segments.append({\n",
    "            'start': start_pos_no_punc,\n",
    "            'end': end_pos_no_punc,\n",
    "            'is_formulaic': sent_row['is_formulaic']\n",
    "        })\n",
    "    \n",
    "    edict_viz_data.append({\n",
    "        'idx': edict_idx,\n",
    "        'title': edict_row['text_title'],\n",
    "        'length': total_length,\n",
    "        'segments': segments\n",
    "    })\n",
    "\n",
    "# Sort by length for better visualization\n",
    "edict_viz_data.sort(key=lambda x: x['length'], reverse=True)\n",
    "\n",
    "# Create the visualization\n",
    "fig, ax = plt.subplots(figsize=(14, max(6, len(edict_viz_data) * 0.6)))\n",
    "\n",
    "y_positions = range(len(edict_viz_data))\n",
    "bar_height = 0.7\n",
    "\n",
    "# Draw bars for each edict\n",
    "for i, edict_data in enumerate(edict_viz_data):\n",
    "    y_pos = len(edict_viz_data) - 1 - i  # Reverse order for top-to-bottom\n",
    "    \n",
    "    # Draw background (full length in light gray)\n",
    "    ax.barh(y_pos, edict_data['length'], height=bar_height, \n",
    "            color='#f0f0f0', edgecolor='gray', linewidth=0.5)\n",
    "    \n",
    "    # Draw formulaic segments in color\n",
    "    for segment in edict_data['segments']:\n",
    "        if segment['is_formulaic']:\n",
    "            seg_start = segment['start']\n",
    "            seg_width = segment['end'] - segment['start']\n",
    "            ax.barh(y_pos, seg_width, left=seg_start, height=bar_height,\n",
    "                   color='#ff6b6b', edgecolor='none')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_yticks(range(len(edict_viz_data)))\n",
    "ax.set_yticklabels([ed['title'] for ed in reversed(edict_viz_data)], fontsize=10, fontproperties=fm.FontProperties(family=chinese_font))\n",
    "ax.set_xlabel('Length (characters, excluding punctuation)', fontsize=12)\n",
    "ax.set_title(f'Edict Lengths and Formulaic Content Distribution - {EDICT_TYPE}', \n",
    "             fontsize=14, fontweight='bold', pad=20, fontproperties=fm.FontProperties(family=chinese_font))\n",
    "\n",
    "# Add grid for readability\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add legend\n",
    "formulaic_patch = mpatches.Patch(color='#ff6b6b', label='Formulaic sentences')\n",
    "unique_patch = mpatches.Patch(color='#f0f0f0', label='Unique sentences')\n",
    "ax.legend(handles=[formulaic_patch, unique_patch], loc='lower right', fontsize=10, prop=fm.FontProperties(family=chinese_font))\n",
    "\n",
    "# Add text annotations showing exact lengths\n",
    "for i, edict_data in enumerate(edict_viz_data):\n",
    "    y_pos = len(edict_viz_data) - 1 - i\n",
    "    # Calculate formulaic character count\n",
    "    formulaic_chars = sum(seg['end'] - seg['start'] \n",
    "                          for seg in edict_data['segments'] \n",
    "                          if seg['is_formulaic'])\n",
    "    unique_chars = edict_data['length'] - formulaic_chars\n",
    "    formulaic_pct = (formulaic_chars / edict_data['length'] * 100) if edict_data['length'] > 0 else 0\n",
    "    \n",
    "    # Add text at the end of the bar\n",
    "    ax.text(edict_data['length'] + 50, y_pos, \n",
    "            f\"{edict_data['length']} chars ({formulaic_pct:.0f}% formulaic)\",\n",
    "            va='center', fontsize=9, color='#555')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"EDICT LENGTH SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n{'Edict Title':<50} {'Total':<10} {'Formulaic':<12} {'Unique':<10} {'% Formulaic':<12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for edict_data in edict_viz_data:\n",
    "    formulaic_chars = sum(seg['end'] - seg['start'] \n",
    "                          for seg in edict_data['segments'] \n",
    "                          if seg['is_formulaic'])\n",
    "    unique_chars = edict_data['length'] - formulaic_chars\n",
    "    formulaic_pct = (formulaic_chars / edict_data['length'] * 100) if edict_data['length'] > 0 else 0\n",
    "    \n",
    "    title_short = edict_data['title'][:47] + '...' if len(edict_data['title']) > 50 else edict_data['title']\n",
    "    print(f\"{title_short:<50} {edict_data['length']:<10} {formulaic_chars:<12} {unique_chars:<10} {formulaic_pct:<12.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"Average edict length: {sum(ed['length'] for ed in edict_viz_data) / len(edict_viz_data):.0f} characters\")\n",
    "print(f\"Shortest edict: {min(ed['length'] for ed in edict_viz_data)} characters\")\n",
    "print(f\"Longest edict: {max(ed['length'] for ed in edict_viz_data)} characters\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tang_edicts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
